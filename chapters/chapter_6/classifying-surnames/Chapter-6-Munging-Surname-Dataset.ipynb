{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"toc":{"colors":{"hover_highlight":"#DAA520","running_highlight":"#FF0000","selected_highlight":"#FFD700"},"moveMenuLeft":true,"nav_menu":{"height":"12px","width":"252px"},"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":"5","toc_cell":false,"toc_section_display":"block","toc_window_display":false},"colab":{"name":"Chapter-6-Munging-Surname-Dataset.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"35gDOhJ1E5sa"},"source":["import collections\n","import numpy as np\n","import pandas as pd\n","import re\n","\n","from argparse import Namespace\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd drive/MyDrive/CSC-project/PyTorchNLPBook/\n","!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooDksB-5E5sc"},"source":["args = Namespace(\n","    raw_dataset_csv=\"data/surnames/surnames.csv\",\n","    train_proportion=0.7,\n","    val_proportion=0.15,\n","    test_proportion=0.15,\n","    output_munged_csv=\"data/surnames/surnames_with_splits.csv\",\n","    seed=1337\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"upaQwjM0E5sd"},"source":["# Read raw data\n","surnames = pd.read_csv(args.raw_dataset_csv, header=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSC_leDaE5se","outputId":"ec30a331-337c-4901-aff8-29d431eff4a2"},"source":["surnames.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>surname</th>\n","      <th>nationality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Woodford</td>\n","      <td>English</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Coté</td>\n","      <td>French</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Kore</td>\n","      <td>English</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Koury</td>\n","      <td>Arabic</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Lebzak</td>\n","      <td>Russian</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    surname nationality\n","0  Woodford     English\n","1      Coté      French\n","2      Kore     English\n","3     Koury      Arabic\n","4    Lebzak     Russian"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"nJqpZltBE5se","outputId":"c59591f7-2edc-4514-8dd0-aaf8406c74f3"},"source":["# Unique classes\n","set(surnames.nationality)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Arabic',\n"," 'Chinese',\n"," 'Czech',\n"," 'Dutch',\n"," 'English',\n"," 'French',\n"," 'German',\n"," 'Greek',\n"," 'Irish',\n"," 'Italian',\n"," 'Japanese',\n"," 'Korean',\n"," 'Polish',\n"," 'Portuguese',\n"," 'Russian',\n"," 'Scottish',\n"," 'Spanish',\n"," 'Vietnamese'}"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Ne6OwK5aE5sf"},"source":["# Splitting train by nationality\n","# Create dict\n","by_nationality = collections.defaultdict(list)\n","for _, row in surnames.iterrows():\n","    by_nationality[row.nationality].append(row.to_dict())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3H-DQL5E5sf"},"source":["# Create split data\n","final_list = []\n","np.random.seed(args.seed)\n","for _, item_list in sorted(by_nationality.items()):\n","    np.random.shuffle(item_list)\n","    n = len(item_list)\n","    n_train = int(args.train_proportion*n)\n","    n_val = int(args.val_proportion*n)\n","    n_test = int(args.test_proportion*n)\n","    \n","    # Give data point a split attribute\n","    for item in item_list[:n_train]:\n","        item['split'] = 'train'\n","    for item in item_list[n_train:n_train+n_val]:\n","        item['split'] = 'val'\n","    for item in item_list[n_train+n_val:]:\n","        item['split'] = 'test'  \n","    \n","    # Add to final list\n","    final_list.extend(item_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"reAdgAhNE5sg"},"source":["# Write split data to file\n","final_surnames = pd.DataFrame(final_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PE9A_lZcE5sg","outputId":"4b3e4117-bf36-42fe-8d38-45f02e385aed"},"source":["final_surnames.split.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["train    7680\n","test     1660\n","val      1640\n","Name: split, dtype: int64"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"35yRf7QlE5sg","outputId":"9f5d7490-0bc9-4719-9348-81035cc136d6"},"source":["final_surnames.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>nationality</th>\n","      <th>split</th>\n","      <th>surname</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Arabic</td>\n","      <td>train</td>\n","      <td>Totah</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Arabic</td>\n","      <td>train</td>\n","      <td>Abboud</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Arabic</td>\n","      <td>train</td>\n","      <td>Fakhoury</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Arabic</td>\n","      <td>train</td>\n","      <td>Srour</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Arabic</td>\n","      <td>train</td>\n","      <td>Sayegh</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  nationality  split   surname\n","0      Arabic  train     Totah\n","1      Arabic  train    Abboud\n","2      Arabic  train  Fakhoury\n","3      Arabic  train     Srour\n","4      Arabic  train    Sayegh"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"bOUU8TJ8E5sh"},"source":["# Write munged data to CSV\n","final_surnames.to_csv(args.output_munged_csv, index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_hq-kGdVE5sh"},"source":[""],"execution_count":null,"outputs":[]}]}