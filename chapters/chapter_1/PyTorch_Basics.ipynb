{"nbformat":4,"nbformat_minor":0,"metadata":{"hide_input":false,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"PyTorch_Basics.ipynb","provenance":[],"collapsed_sections":["IE7eI2WLM85Q","koniPwWUM85V","BVPWyhhtM85Z","wgi7VAOQM85c","29N4uVxzM85h","nkg0vakRM85l","3O4dgIkBM85n","K2Rse-GNM85n","u0WgJbOzM85o","UlP8NLjNM85o","fC_TkpmCM85o","d1C5oFy-M85p","J26GpKCsM85p","3EVffFVKM85p","GSN-Nk3PM85q","li75nTTBM85s","Tzt1qrqCM85t","p-Gf6i13M85t","dyvJZJ9YM85u","0nj0pKJrM85u","YQhd6RW_M85v","Y_h-BhlqM85v","CGOZX8KiM85w","0-IYZNP-M85w"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xkZBPDM4M85C"},"source":["# PyTorch Basics"]},{"cell_type":"code","metadata":{"id":"K0U48EoCM85I","outputId":"7200dab3-2fe4-47ab-95d2-6b9b760179c4"},"source":["import torch\n","import numpy as np\n","torch.manual_seed(1234)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f35ea8a2630>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"WjTOgfZ0M85K"},"source":["## Tensors"]},{"cell_type":"markdown","metadata":{"id":"bG8B4YDjM85K"},"source":["* Scalar is a single number.\n","* Vector is an array of numbers.\n","* Matrix is a 2-D array of numbers.\n","* Tensors are N-D arrays of numbers."]},{"cell_type":"markdown","metadata":{"id":"pg8FctXEM85L"},"source":["#### Creating Tensors"]},{"cell_type":"markdown","metadata":{"id":"PPHihoL_M85L"},"source":["You can create tensors by specifying the shape as arguments.  Here is a tensor with 5 rows and 3 columns"]},{"cell_type":"code","metadata":{"id":"rn7MbiuuM85L"},"source":["def describe(x):\n","    print(\"Type: {}\".format(x.type()))\n","    print(\"Shape/size: {}\".format(x.shape))\n","    print(\"Values: \\n{}\".format(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fey3wcnjM85M","outputId":"9739e8e2-4930-42db-c793-6b6009b0214c"},"source":["describe(torch.Tensor(2, 3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[ 3.1654e+09,  4.5635e-41, -5.4825e-21],\n","        [ 3.0718e-41,  4.4842e-44,  0.0000e+00]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q1Mpoc-QM85M","outputId":"625644b8-b7a4-4390-9a57-94f440939c58"},"source":["describe(torch.randn(2, 3))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[ 0.0461,  0.4024, -1.0115],\n","        [ 0.2167, -0.6123,  0.5036]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F0qwOr7cM85N"},"source":["It's common in prototyping to create a tensor with random numbers of a specific shape."]},{"cell_type":"code","metadata":{"id":"UoJOa1KKM85N","outputId":"2831c4b3-94dc-4e70-c9b8-54f72d3a6560"},"source":["x = torch.rand(2, 3)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0.7749, 0.8208, 0.2793],\n","        [0.6817, 0.2837, 0.6567]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PSO-VhFiM85O"},"source":["You can also initialize tensors of ones or zeros."]},{"cell_type":"code","metadata":{"id":"Unc5o7lNM85O","outputId":"e8482640-a17d-4502-f3cb-33e0aeaf7061"},"source":["describe(torch.zeros(2, 3))\n","x = torch.ones(2, 3)\n","describe(x)\n","x.fill_(5)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[5., 5., 5.],\n","        [5., 5., 5.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZrXa1j1uM85O"},"source":["Tensors can be initialized and then filled in place. \n","\n","Note: operations that end in an underscore (`_`) are in place operations."]},{"cell_type":"code","metadata":{"id":"Rkh1CwO_M85P","outputId":"1bd93c47-d198-43e6-ce89-de0da711cd5e"},"source":["x = torch.Tensor(3,4).fill_(5)\n","print(x.type())\n","print(x.shape)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.FloatTensor\n","torch.Size([3, 4])\n","tensor([[5., 5., 5., 5.],\n","        [5., 5., 5., 5.],\n","        [5., 5., 5., 5.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XzZxWsxOM85P"},"source":["Tensors can be initialized from a list of lists"]},{"cell_type":"code","metadata":{"id":"XtpyD7NnM85P","outputId":"bfe098d9-3aca-4202-973f-fa123e568461"},"source":["x = torch.Tensor([[1, 2,],  \n","                  [2, 4,]])\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 2])\n","Values: \n","tensor([[1., 2.],\n","        [2., 4.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NBYe6-beM85P"},"source":["Tensors can be initialized from numpy matrices"]},{"cell_type":"code","metadata":{"id":"DtJ79LXqM85Q","outputId":"036ed4ca-c2f8-4168-eb63-14b96aa60d9f"},"source":["npy = np.random.rand(2, 3)\n","describe(torch.from_numpy(npy))\n","print(npy.dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.DoubleTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0.6938, 0.0125, 0.7894],\n","        [0.4493, 0.1734, 0.4403]], dtype=torch.float64)\n","float64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IE7eI2WLM85Q"},"source":["#### Tensor Types"]},{"cell_type":"markdown","metadata":{"id":"HVRrLwCxM85Q"},"source":["The FloatTensor has been the default tensor that we have been creating all along"]},{"cell_type":"code","metadata":{"id":"bzW3aWz8M85Q","outputId":"071ce298-6d65-42ad-b9c5-b773456eb24d"},"source":["import torch\n","x = torch.arange(6).view(2, 3)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [3, 4, 5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gVIU3ojeM85R","outputId":"859837c3-97c6-4b4b-a6ed-71fb9559a8f7"},"source":["x = torch.FloatTensor([[1, 2, 3],  \n","                       [4, 5, 6]])\n","describe(x)\n","\n","x = x.long()\n","describe(x)\n","\n","x = torch.tensor([[1, 2, 3], \n","                  [4, 5, 6]], dtype=torch.int64)\n","describe(x)\n","\n","x = x.float() \n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[1, 2, 3],\n","        [4, 5, 6]])\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[1., 2., 3.],\n","        [4., 5., 6.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tmHkdBD8M85R","outputId":"43cd6960-70d4-48a9-8486-10dfbf67e8d6"},"source":["x = torch.randn(2, 3)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[ 1.5385, -0.9757,  1.5769],\n","        [ 0.3840, -0.6039, -0.5240]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aN4BHTdpM85R","outputId":"94d5f262-5bcf-43e2-c816-873200c2a12a"},"source":["describe(torch.add(x, x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[ 3.0771, -1.9515,  3.1539],\n","        [ 0.7680, -1.2077, -1.0479]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sC9Po65jM85S","outputId":"48d749a5-ad9f-4c89-bf06-a2e6e9162f58"},"source":["describe(x + x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[ 3.0771, -1.9515,  3.1539],\n","        [ 0.7680, -1.2077, -1.0479]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4G0EKqWiM85S","outputId":"94bed07b-78b1-43ec-b98c-5892ce3ff217"},"source":["x = torch.arange(6)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([6])\n","Values: \n","tensor([0, 1, 2, 3, 4, 5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C6FChNDcM85S","outputId":"7e1500f1-96fb-418c-aaa9-b055247d41f9"},"source":["x = x.view(2, 3)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [3, 4, 5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FLxHsc-uM85S","outputId":"939d206c-75cc-4d50-c2e6-e97658c2cd42"},"source":["describe(torch.sum(x, dim=0))\n","describe(torch.sum(x, dim=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([3])\n","Values: \n","tensor([3, 5, 7])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([2])\n","Values: \n","tensor([ 3, 12])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"INXsuEDhM85T","outputId":"0f945372-1824-40db-d891-fc5f328414e8"},"source":["describe(torch.transpose(x, 0, 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([3, 2])\n","Values: \n","tensor([[0, 3],\n","        [1, 4],\n","        [2, 5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ej_hHBwiM85T","outputId":"f1f967d0-d8bf-4765-8322-bc3d33d20ff8"},"source":["import torch\n","x = torch.arange(6).view(2, 3)\n","describe(x)\n","describe(x[:1, :2])\n","describe(x[0, 1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [3, 4, 5]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([1, 2])\n","Values: \n","tensor([[0, 1]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([])\n","Values: \n","1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NVXqb-GuM85T","outputId":"d85a0086-348c-4616-fc8e-45413550cf84"},"source":["indices = torch.LongTensor([0, 2])\n","describe(torch.index_select(x, dim=1, index=indices))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 2])\n","Values: \n","tensor([[0, 2],\n","        [3, 5]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"biMTdaqeM85T","outputId":"29cd7fb1-2a5e-4f3e-c70c-a6b3d704fe84"},"source":["indices = torch.LongTensor([0, 0])\n","describe(torch.index_select(x, dim=0, index=indices))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [0, 1, 2]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q9OCjf_0M85U","outputId":"6cb6ab59-cae6-4661-faa3-33dc33cfde4e"},"source":["row_indices = torch.arange(2).long()\n","col_indices = torch.LongTensor([0, 1])\n","describe(x[row_indices, col_indices])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2])\n","Values: \n","tensor([0, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3dLSEURXM85U"},"source":["Long Tensors are used for indexing operations and mirror the `int64` numpy type"]},{"cell_type":"code","metadata":{"id":"FPSGyt7pM85U","outputId":"0fa44354-94d9-4661-88ed-b1b6580f2219"},"source":["x = torch.LongTensor([[1, 2, 3],  \n","                      [4, 5, 6],\n","                      [7, 8, 9]])\n","describe(x)\n","print(x.dtype)\n","print(x.numpy().dtype)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([3, 3])\n","Values: \n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","torch.int64\n","int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C8y693i3M85U"},"source":["You can convert a FloatTensor to a LongTensor"]},{"cell_type":"code","metadata":{"id":"ggMwodfgM85U","outputId":"a481b433-9e0c-49b4-aa1a-36a03b94cb33"},"source":["x = torch.FloatTensor([[1, 2, 3],  \n","                       [4, 5, 6],\n","                       [7, 8, 9]])\n","x = x.long()\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([3, 3])\n","Values: \n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"koniPwWUM85V"},"source":["### Special Tensor initializations"]},{"cell_type":"markdown","metadata":{"id":"1RYdAf6aM85V"},"source":["We can create a vector of incremental numbers"]},{"cell_type":"code","metadata":{"id":"37U5C1WVM85V","outputId":"e936f82f-d83d-4354-ac43-5aff65815685"},"source":["x = torch.arange(0, 10)\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yi1-AMVfM85V"},"source":["Sometimes it's useful to have an integer-based arange for indexing"]},{"cell_type":"code","metadata":{"id":"YfuoT6BEM85V","outputId":"5b6f339c-4318-476c-9879-c4b3fd014a31"},"source":["x = torch.arange(0, 10).long()\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NjgkSs84M85W"},"source":["## Operations\n","\n","Using the tensors to do linear algebra is a foundation of modern Deep Learning practices"]},{"cell_type":"markdown","metadata":{"id":"cL5ZBwaeM85W"},"source":["Reshaping allows you to move the numbers in a tensor around.  One can be sure that the order is preserved.  In PyTorch, reshaping is called `view`"]},{"cell_type":"code","metadata":{"id":"9MsdfwgQM85W","outputId":"751b0b0e-96c5-4036-a6ff-113cf1bf9049"},"source":["x = torch.arange(0, 20)\n","\n","print(x.view(1, 20))\n","print(x.view(2, 10))\n","print(x.view(4, 5))\n","print(x.view(5, 4))\n","print(x.view(10, 2))\n","print(x.view(20, 1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","         18, 19]])\n","tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])\n","tensor([[ 0,  1,  2,  3,  4],\n","        [ 5,  6,  7,  8,  9],\n","        [10, 11, 12, 13, 14],\n","        [15, 16, 17, 18, 19]])\n","tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11],\n","        [12, 13, 14, 15],\n","        [16, 17, 18, 19]])\n","tensor([[ 0,  1],\n","        [ 2,  3],\n","        [ 4,  5],\n","        [ 6,  7],\n","        [ 8,  9],\n","        [10, 11],\n","        [12, 13],\n","        [14, 15],\n","        [16, 17],\n","        [18, 19]])\n","tensor([[ 0],\n","        [ 1],\n","        [ 2],\n","        [ 3],\n","        [ 4],\n","        [ 5],\n","        [ 6],\n","        [ 7],\n","        [ 8],\n","        [ 9],\n","        [10],\n","        [11],\n","        [12],\n","        [13],\n","        [14],\n","        [15],\n","        [16],\n","        [17],\n","        [18],\n","        [19]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"O0z9CmQuM85W"},"source":["We can use view to add size-1 dimensions, which can be useful for combining with other tensors.  This is called broadcasting. "]},{"cell_type":"code","metadata":{"id":"79q7xsFUM85X","outputId":"86240779-8fbb-4817-80df-61cc615db3e0"},"source":["x = torch.arange(12).view(3, 4)\n","y = torch.arange(4).view(1, 4)\n","z = torch.arange(3).view(3, 1)\n","\n","print(x)\n","print(y)\n","print(z)\n","print(x + y)\n","print(x + z)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","tensor([[0, 1, 2, 3]])\n","tensor([[0],\n","        [1],\n","        [2]])\n","tensor([[ 0,  2,  4,  6],\n","        [ 4,  6,  8, 10],\n","        [ 8, 10, 12, 14]])\n","tensor([[ 0,  1,  2,  3],\n","        [ 5,  6,  7,  8],\n","        [10, 11, 12, 13]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JeqKg_FsM85X"},"source":["Unsqueeze and squeeze will add and remove 1-dimensions."]},{"cell_type":"code","metadata":{"id":"4CsdEcbEM85X","outputId":"210e2a9e-5df0-49d2-cada-d10ab47eb2e9"},"source":["x = torch.arange(12).view(3, 4)\n","print(x.shape)\n","\n","x = x.unsqueeze(dim=1)\n","print(x.shape)\n","\n","x = x.squeeze()\n","print(x.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([3, 4])\n","torch.Size([3, 1, 4])\n","torch.Size([3, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ejnls89SM85X"},"source":["all of the standard mathematics operations apply (such as `add` below)"]},{"cell_type":"code","metadata":{"id":"5Hr-wdHGM85X","outputId":"f154ef3d-4b30-4dac-9c42-ddb9d7dfcd84"},"source":["x = torch.rand(3,4)\n","print(\"x: \\n\", x)\n","print(\"--\")\n","print(\"torch.add(x, x): \\n\", torch.add(x, x))\n","print(\"--\")\n","print(\"x+x: \\n\", x + x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x: \n"," tensor([[0.6662, 0.3343, 0.7893, 0.3216],\n","        [0.5247, 0.6688, 0.8436, 0.4265],\n","        [0.9561, 0.0770, 0.4108, 0.0014]])\n","--\n","torch.add(x, x): \n"," tensor([[1.3324, 0.6686, 1.5786, 0.6433],\n","        [1.0494, 1.3377, 1.6872, 0.8530],\n","        [1.9123, 0.1540, 0.8216, 0.0028]])\n","--\n","x+x: \n"," tensor([[1.3324, 0.6686, 1.5786, 0.6433],\n","        [1.0494, 1.3377, 1.6872, 0.8530],\n","        [1.9123, 0.1540, 0.8216, 0.0028]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MF6du0geM85Y"},"source":["The convention of `_` indicating in-place operations continues:"]},{"cell_type":"code","metadata":{"id":"U3r9kOmMM85Y","outputId":"ef273387-8d80-4374-f93b-13a47ea62377"},"source":["x = torch.arange(12).reshape(3, 4)\n","print(x)\n","print(x.add_(x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","tensor([[ 0,  2,  4,  6],\n","        [ 8, 10, 12, 14],\n","        [16, 18, 20, 22]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3R06UBdSM85Y"},"source":["There are many operations for which reduce a dimension.  Such as sum:"]},{"cell_type":"code","metadata":{"id":"o3S3PgzfM85Y","outputId":"69121ae1-bba5-429c-a117-2d9a32b4757c"},"source":["x = torch.arange(12).reshape(3, 4)\n","print(\"x: \\n\", x)\n","print(\"---\")\n","print(\"Summing across rows (dim=0): \\n\", x.sum(dim=0))\n","print(\"---\")\n","print(\"Summing across columns (dim=1): \\n\", x.sum(dim=1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x: \n"," tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","---\n","Summing across rows (dim=0): \n"," tensor([12, 15, 18, 21])\n","---\n","Summing across columns (dim=1): \n"," tensor([ 6, 22, 38])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BVPWyhhtM85Z"},"source":["#### Indexing, Slicing, Joining and Mutating"]},{"cell_type":"code","metadata":{"id":"MftfeunDM85Z","outputId":"79797f4e-3d95-4081-c17a-b05920af014f"},"source":["x = torch.arange(6).view(2, 3)\n","print(\"x: \\n\", x)\n","print(\"---\")\n","print(\"x[:2, :2]: \\n\", x[:2, :2])\n","print(\"---\")\n","print(\"x[0][1]: \\n\", x[0][1])\n","print(\"---\")\n","print(\"Setting [0][1] to be 8\")\n","x[0][1] = 8\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x: \n"," tensor([[0, 1, 2],\n","        [3, 4, 5]])\n","---\n","x[:2, :2]: \n"," tensor([[0, 1],\n","        [3, 4]])\n","---\n","x[0][1]: \n"," tensor(1)\n","---\n","Setting [0][1] to be 8\n","tensor([[0, 8, 2],\n","        [3, 4, 5]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xWqfLOYeM85Z"},"source":["We can select a subset of a tensor using the `index_select`"]},{"cell_type":"code","metadata":{"id":"vvMtlDMaM85Z","outputId":"272843a0-52db-44e9-ff8d-f8d7bd786ae9"},"source":["x = torch.arange(9).view(3,3)\n","print(x)\n","\n","print(\"---\")\n","indices = torch.LongTensor([0, 2])\n","print(torch.index_select(x, dim=0, index=indices))\n","\n","print(\"---\")\n","indices = torch.LongTensor([0, 2])\n","print(torch.index_select(x, dim=1, index=indices))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])\n","---\n","tensor([[0, 1, 2],\n","        [6, 7, 8]])\n","---\n","tensor([[0, 2],\n","        [3, 5],\n","        [6, 8]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MMaxc1G0M85a"},"source":["We can also use numpy-style advanced indexing:"]},{"cell_type":"code","metadata":{"id":"FQIML39tM85a","outputId":"8b4ce864-621b-4996-e3ef-c96c19b401d6"},"source":["x = torch.arange(9).view(3,3)\n","indices = torch.LongTensor([0, 2])\n","\n","print(x[indices])\n","print(\"---\")\n","print(x[indices, :])\n","print(\"---\")\n","print(x[:, indices])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0, 1, 2],\n","        [6, 7, 8]])\n","---\n","tensor([[0, 1, 2],\n","        [6, 7, 8]])\n","---\n","tensor([[0, 2],\n","        [3, 5],\n","        [6, 8]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R9kdNJBuM85a"},"source":["We can combine tensors by concatenating them.  First, concatenating on the rows"]},{"cell_type":"code","metadata":{"id":"kiHF6knXM85a","outputId":"687df10f-fd58-4760-a53c-c20f4d0d51de"},"source":["x = torch.arange(6).view(2,3)\n","describe(x)\n","describe(torch.cat([x, x], dim=0))\n","describe(torch.cat([x, x], dim=1))\n","describe(torch.stack([x, x]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.LongTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [3, 4, 5]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([4, 3])\n","Values: \n","tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [0, 1, 2],\n","        [3, 4, 5]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([2, 6])\n","Values: \n","tensor([[0, 1, 2, 0, 1, 2],\n","        [3, 4, 5, 3, 4, 5]])\n","Type: torch.LongTensor\n","Shape/size: torch.Size([2, 2, 3])\n","Values: \n","tensor([[[0, 1, 2],\n","         [3, 4, 5]],\n","\n","        [[0, 1, 2],\n","         [3, 4, 5]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AHK2kb3tM85b"},"source":["We can concentate along the first dimension.. the columns."]},{"cell_type":"code","metadata":{"id":"RRMatNsqM85b","outputId":"bf79acd3-ffe3-413d-f84d-eb5086ff6248"},"source":["x = torch.arange(9).view(3,3)\n","\n","print(x)\n","print(\"---\")\n","new_x = torch.cat([x, x, x], dim=1)\n","print(new_x.shape)\n","print(new_x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])\n","---\n","torch.Size([3, 9])\n","tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2],\n","        [3, 4, 5, 3, 4, 5, 3, 4, 5],\n","        [6, 7, 8, 6, 7, 8, 6, 7, 8]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OzSC-HrGM85b"},"source":["We can also concatenate on a new 0th dimension to \"stack\" the tensors:"]},{"cell_type":"code","metadata":{"id":"qWlBIZcSM85c","outputId":"064dc90f-641f-4e4f-fd44-b4b505b945d9"},"source":["x = torch.arange(9).view(3,3)\n","print(x)\n","print(\"---\")\n","new_x = torch.stack([x, x, x])\n","print(new_x.shape)\n","print(new_x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0, 1, 2],\n","        [3, 4, 5],\n","        [6, 7, 8]])\n","---\n","torch.Size([3, 3, 3])\n","tensor([[[0, 1, 2],\n","         [3, 4, 5],\n","         [6, 7, 8]],\n","\n","        [[0, 1, 2],\n","         [3, 4, 5],\n","         [6, 7, 8]],\n","\n","        [[0, 1, 2],\n","         [3, 4, 5],\n","         [6, 7, 8]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wgi7VAOQM85c"},"source":["#### Linear Algebra Tensor Functions"]},{"cell_type":"markdown","metadata":{"id":"jYWO7OSDM85d"},"source":["Transposing allows you to switch the dimensions to be on different axis. So we can make it so all the rows are columsn and vice versa. "]},{"cell_type":"code","metadata":{"id":"6McExnJNM85d","outputId":"a5330546-f642-43c6-ec9e-abea98d0b521"},"source":["x = torch.arange(0, 12).view(3,4)\n","print(\"x: \\n\", x) \n","print(\"---\")\n","print(\"x.tranpose(1, 0): \\n\", x.transpose(1, 0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x: \n"," tensor([[ 0,  1,  2,  3],\n","        [ 4,  5,  6,  7],\n","        [ 8,  9, 10, 11]])\n","---\n","x.tranpose(1, 0): \n"," tensor([[ 0,  4,  8],\n","        [ 1,  5,  9],\n","        [ 2,  6, 10],\n","        [ 3,  7, 11]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zl9NBnSTM85d"},"source":["A three dimensional tensor would represent a batch of sequences, where each sequence item has a feature vector.  It is common to switch the batch and sequence dimensions so that we can more easily index the sequence in a sequence model. \n","\n","Note: Transpose will only let you swap 2 axes.  Permute (in the next cell) allows for multiple"]},{"cell_type":"code","metadata":{"id":"W4D2P2lpM85e","outputId":"99b22534-d620-4c58-be1a-bd8e4bf97a93"},"source":["batch_size = 3\n","seq_size = 4\n","feature_size = 5\n","\n","x = torch.arange(batch_size * seq_size * feature_size).view(batch_size, seq_size, feature_size)\n","\n","print(\"x.shape: \\n\", x.shape)\n","print(\"x: \\n\", x)\n","print(\"-----\")\n","\n","print(\"x.transpose(1, 0).shape: \\n\", x.transpose(1, 0).shape)\n","print(\"x.transpose(1, 0): \\n\", x.transpose(1, 0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x.shape: \n"," torch.Size([3, 4, 5])\n","x: \n"," tensor([[[ 0,  1,  2,  3,  4],\n","         [ 5,  6,  7,  8,  9],\n","         [10, 11, 12, 13, 14],\n","         [15, 16, 17, 18, 19]],\n","\n","        [[20, 21, 22, 23, 24],\n","         [25, 26, 27, 28, 29],\n","         [30, 31, 32, 33, 34],\n","         [35, 36, 37, 38, 39]],\n","\n","        [[40, 41, 42, 43, 44],\n","         [45, 46, 47, 48, 49],\n","         [50, 51, 52, 53, 54],\n","         [55, 56, 57, 58, 59]]])\n","-----\n","x.transpose(1, 0).shape: \n"," torch.Size([4, 3, 5])\n","x.transpose(1, 0): \n"," tensor([[[ 0,  1,  2,  3,  4],\n","         [20, 21, 22, 23, 24],\n","         [40, 41, 42, 43, 44]],\n","\n","        [[ 5,  6,  7,  8,  9],\n","         [25, 26, 27, 28, 29],\n","         [45, 46, 47, 48, 49]],\n","\n","        [[10, 11, 12, 13, 14],\n","         [30, 31, 32, 33, 34],\n","         [50, 51, 52, 53, 54]],\n","\n","        [[15, 16, 17, 18, 19],\n","         [35, 36, 37, 38, 39],\n","         [55, 56, 57, 58, 59]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rX14wq-NM85e"},"source":["Permute is a more general version of tranpose:"]},{"cell_type":"code","metadata":{"id":"Rrfq8eESM85e","outputId":"41bbc310-e5aa-4ddd-b29a-1ebf95690b10"},"source":["batch_size = 3\n","seq_size = 4\n","feature_size = 5\n","\n","x = torch.arange(batch_size * seq_size * feature_size).view(batch_size, seq_size, feature_size)\n","\n","print(\"x.shape: \\n\", x.shape)\n","print(\"x: \\n\", x)\n","print(\"-----\")\n","\n","print(\"x.permute(1, 0, 2).shape: \\n\", x.permute(1, 0, 2).shape)\n","print(\"x.permute(1, 0, 2): \\n\", x.permute(1, 0, 2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x.shape: \n"," torch.Size([3, 4, 5])\n","x: \n"," tensor([[[ 0,  1,  2,  3,  4],\n","         [ 5,  6,  7,  8,  9],\n","         [10, 11, 12, 13, 14],\n","         [15, 16, 17, 18, 19]],\n","\n","        [[20, 21, 22, 23, 24],\n","         [25, 26, 27, 28, 29],\n","         [30, 31, 32, 33, 34],\n","         [35, 36, 37, 38, 39]],\n","\n","        [[40, 41, 42, 43, 44],\n","         [45, 46, 47, 48, 49],\n","         [50, 51, 52, 53, 54],\n","         [55, 56, 57, 58, 59]]])\n","-----\n","x.permute(1, 0, 2).shape: \n"," torch.Size([4, 3, 5])\n","x.permute(1, 0, 2): \n"," tensor([[[ 0,  1,  2,  3,  4],\n","         [20, 21, 22, 23, 24],\n","         [40, 41, 42, 43, 44]],\n","\n","        [[ 5,  6,  7,  8,  9],\n","         [25, 26, 27, 28, 29],\n","         [45, 46, 47, 48, 49]],\n","\n","        [[10, 11, 12, 13, 14],\n","         [30, 31, 32, 33, 34],\n","         [50, 51, 52, 53, 54]],\n","\n","        [[15, 16, 17, 18, 19],\n","         [35, 36, 37, 38, 39],\n","         [55, 56, 57, 58, 59]]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rS6a234CM85f"},"source":["Matrix multiplication is `mm`:"]},{"cell_type":"code","metadata":{"id":"ta9uzyB2M85f","outputId":"c909d3ce-6027-4ab8-9e71-22e26eaccb31"},"source":["torch.randn(2, 3, requires_grad=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.4790,  0.8539, -0.2285],\n","        [ 0.3081,  1.1171,  0.1585]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"FQLnHNDbM85f","outputId":"6c932186-1e97-4367-d789-45622379ad0b"},"source":["x1 = torch.arange(6).view(2, 3).float()\n","describe(x1)\n","\n","x2 = torch.ones(3, 2)\n","x2[:, 1] += 1\n","describe(x2)\n","\n","describe(torch.mm(x1, x2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 3])\n","Values: \n","tensor([[0., 1., 2.],\n","        [3., 4., 5.]])\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([3, 2])\n","Values: \n","tensor([[1., 2.],\n","        [1., 2.],\n","        [1., 2.]])\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 2])\n","Values: \n","tensor([[ 3.,  6.],\n","        [12., 24.]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iKx7uifbM85f","outputId":"0f239034-200b-4e4b-f819-8955844ebd1c"},"source":["x = torch.arange(0, 12).view(3,4).float()\n","print(x)\n","\n","x2 = torch.ones(4, 2)\n","x2[:, 1] += 1\n","print(x2)\n","\n","print(x.mm(x2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 0.,  1.,  2.,  3.],\n","        [ 4.,  5.,  6.,  7.],\n","        [ 8.,  9., 10., 11.]])\n","tensor([[1., 2.],\n","        [1., 2.],\n","        [1., 2.],\n","        [1., 2.]])\n","tensor([[ 6., 12.],\n","        [22., 44.],\n","        [38., 76.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"opo8sXiHM85g"},"source":["See the [PyTorch Math Operations Documentation](https://pytorch.org/docs/stable/torch.html#math-operations) for more!"]},{"cell_type":"markdown","metadata":{"id":"Z-ETpXqeM85g"},"source":["## Computing Gradients"]},{"cell_type":"code","metadata":{"id":"FI0FaHyGM85g","outputId":"a9d2783d-9103-44cb-9d07-bd766aa2fd47"},"source":["x = torch.tensor([[2.0, 3.0]], requires_grad=True)\n","z = 3 * x\n","print(z)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[6., 9.]], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OvGK6exmM85g"},"source":["In this small snippet, you can see the gradient computations at work.  We create a tensor and multiply it by 3.  Then, we create a scalar output using `sum()`.  A Scalar output is needed as the the loss variable. Then, called backward on the loss means it computes its rate of change with respect to the inputs.  Since the scalar was created with sum, each position in z and x are independent with respect to the loss scalar. \n","\n","The rate of change of x with respect to the output is just the constant 3 that we multiplied x by."]},{"cell_type":"code","metadata":{"id":"rLk_AMYYM85g","outputId":"5fdd61fb-2ef3-4c18-f6d0-b7d65b621ff3"},"source":["x = torch.tensor([[2.0, 3.0]], requires_grad=True)\n","print(\"x: \\n\", x)\n","print(\"---\")\n","z = 3 * x\n","print(\"z = 3*x: \\n\", z)\n","print(\"---\")\n","\n","loss = z.sum()\n","print(\"loss = z.sum(): \\n\", loss)\n","print(\"---\")\n","\n","loss.backward()\n","\n","print(\"after loss.backward(), x.grad: \\n\", x.grad)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["x: \n"," tensor([[2., 3.]], requires_grad=True)\n","---\n","z = 3*x: \n"," tensor([[6., 9.]], grad_fn=<MulBackward0>)\n","---\n","loss = z.sum(): \n"," tensor(15., grad_fn=<SumBackward0>)\n","---\n","after loss.backward(), x.grad: \n"," tensor([[3., 3.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"29N4uVxzM85h"},"source":["### Example: Computing a conditional gradient\n","\n","$$ \\text{ Find the gradient of f(x) at x=1 } $$\n","$$ {} $$\n","$$ f(x)=\\left\\{\n","\\begin{array}{ll}\n","    sin(x) \\text{ if } x>0 \\\\\n","    cos(x) \\text{ otherwise } \\\\\n","\\end{array}\n","\\right.$$"]},{"cell_type":"code","metadata":{"id":"igxbPbhFM85h"},"source":["def f(x):\n","    if (x.data > 0).all():\n","        return torch.sin(x)\n","    else:\n","        return torch.cos(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sO2yL6yuM85h","outputId":"095a33b3-e477-488c-f4d2-5ed16aa45130"},"source":["x = torch.tensor([1.0], requires_grad=True)\n","y = f(x)\n","y.backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.5403])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CtUapWlzM85i"},"source":["We could apply this to a larger vector too, but we need to make sure the output is a scalar:"]},{"cell_type":"code","metadata":{"id":"g3dL3kyVM85i","outputId":"918703b0-9370-4ce5-9f60-5fbc50a5c485"},"source":["x = torch.tensor([1.0, 0.5], requires_grad=True)\n","y = f(x)\n","# this is meant to break!\n","y.backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"grad can be implicitly created only for scalar outputs","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-48e74398a3f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/nlpbook/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"markdown","metadata":{"id":"sXUT6tk8M85i"},"source":["Making the output a scalar:"]},{"cell_type":"code","metadata":{"id":"yeMeFO05M85i","outputId":"27812994-baa3-4f85-d584-b1141e74763d"},"source":["x = torch.tensor([1.0, 0.5], requires_grad=True)\n","y = f(x)\n","y.sum().backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.5403, 0.8776])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZvOHBlhXM85j"},"source":["but there was an issue.. this isn't right for this edge case:"]},{"cell_type":"code","metadata":{"id":"d3ojgNQLM85j","outputId":"090dacc7-8ae9-4657-86d0-0b110f14e192"},"source":["x = torch.tensor([1.0, -1], requires_grad=True)\n","y = f(x)\n","y.sum().backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-0.8415,  0.8415])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZTRp6lw9M85j","outputId":"a9d36ecc-f0ee-4b1a-8873-5065fde6695b"},"source":["x = torch.tensor([-0.5, -1], requires_grad=True)\n","y = f(x)\n","y.sum().backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.4794, 0.8415])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XSTpbt9rM85j"},"source":["This is because we aren't doing the boolean computation and subsequent application of cos and sin on an elementwise basis.  So, to solve this, it is common to use masking:"]},{"cell_type":"code","metadata":{"id":"St4tU6qdM85k","outputId":"13831542-8a55-48d2-ddd5-78adc1378018"},"source":["def f2(x):\n","    mask = torch.gt(x, 0).float()\n","    return mask * torch.sin(x) + (1 - mask) * torch.cos(x)\n","\n","x = torch.tensor([1.0, -1], requires_grad=True)\n","y = f2(x)\n","y.sum().backward()\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.5403, 0.8415])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Txlg7ZZkM85k"},"source":["def describe_grad(x):\n","    if x.grad is None:\n","        print(\"No gradient information\")\n","    else:\n","        print(\"Gradient: \\n{}\".format(x.grad))\n","        print(\"Gradient Function: {}\".format(x.grad_fn))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QzTV7P-1M85k"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sPX1UniM85k","outputId":"5cf0db4d-ae79-4070-acf1-59f2f503c885"},"source":["import torch\n","x = torch.ones(2, 2, requires_grad=True)\n","describe(x)\n","describe_grad(x)\n","print(\"--------\")\n","\n","y = (x + 2) * (x + 5) + 3\n","describe(y)\n","z = y.mean()\n","describe(z)\n","describe_grad(x)\n","print(\"--------\")\n","z.backward(create_graph=True, retain_graph=True)\n","describe_grad(x)\n","print(\"--------\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 2])\n","Values: \n","tensor([[1., 1.],\n","        [1., 1.]], requires_grad=True)\n","No gradient information\n","--------\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([2, 2])\n","Values: \n","tensor([[21., 21.],\n","        [21., 21.]], grad_fn=<AddBackward0>)\n","Type: torch.FloatTensor\n","Shape/size: torch.Size([])\n","Values: \n","21.0\n","No gradient information\n","--------\n","Gradient: \n","tensor([[2.2500, 2.2500],\n","        [2.2500, 2.2500]], grad_fn=<CloneBackward>)\n","Gradient Function: None\n","--------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bjByrEEEM85k"},"source":["x = torch.ones(2, 2, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"erqW2IXRM85k"},"source":["y = x + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsCXZEVlM85l","outputId":"c07c7692-8df0-4530-a62a-ee3bbfff2e5b"},"source":["y.grad_fn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7f35ea134940>"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"nkg0vakRM85l"},"source":["### CUDA Tensors"]},{"cell_type":"markdown","metadata":{"id":"rY2KOTVMM85l"},"source":["PyTorch's operations can seamlessly be used on the GPU or on the CPU.  There are a couple basic operations for interacting in this way."]},{"cell_type":"code","metadata":{"id":"MiyLerSGM85l","outputId":"926bea98-101b-43fa-ded8-7980bfd9db12"},"source":["print(torch.cuda.is_available())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TBeCdtU3M85m","outputId":"a46194ff-009f-4fe9-815c-981b67ac4620"},"source":["x = torch.rand(3,3)\n","describe(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.FloatTensor\n","Shape/size: torch.Size([3, 3])\n","Values: \n","tensor([[0.9149, 0.3993, 0.1100],\n","        [0.2541, 0.4333, 0.4451],\n","        [0.4966, 0.7865, 0.6604]])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oSZLVHK0M85m","outputId":"4aa1bf01-44a1-4d5a-ad90-698aa4ed604d"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"goaB4ahfM85m","outputId":"4e3203f9-6ff0-4738-afe9-55bde7e16da3"},"source":["x = torch.rand(3, 3).to(device)\n","describe(x)\n","print(x.device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Type: torch.cuda.FloatTensor\n","Shape/size: torch.Size([3, 3])\n","Values: \n","tensor([[0.1303, 0.3498, 0.3824],\n","        [0.8043, 0.3186, 0.2908],\n","        [0.4196, 0.3728, 0.3769]], device='cuda:0')\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rVlPW5ctM85m"},"source":["cpu_device = torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUgEvcbcM85m","outputId":"32b9d3b7-a243-4f9b-b07f-0397142e2ef5"},"source":["# this will break!\n","y = torch.rand(3, 3)\n","x + y"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"expected type torch.cuda.FloatTensor but got torch.FloatTensor","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-80-bc65a5d8cb7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this will break!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: expected type torch.cuda.FloatTensor but got torch.FloatTensor"]}]},{"cell_type":"code","metadata":{"id":"aRQ1hgJ5M85n","outputId":"a43f6a0f-88c3-4c99-9862-f501952b4fce"},"source":["y = y.to(cpu_device)\n","x = x.to(cpu_device)\n","x + y"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8394, 0.5273, 0.8267],\n","        [0.9273, 1.2824, 1.0603],\n","        [0.4574, 0.5968, 1.0541]])"]},"metadata":{"tags":[]},"execution_count":81}]},{"cell_type":"code","metadata":{"id":"j4nl4KSbM85n","outputId":"81d6d545-7523-4eb7-a057-619a2a72ba94"},"source":["if torch.cuda.is_available(): # only is GPU is available\n","    a = torch.rand(3,3).to(device='cuda:0') #  CUDA Tensor\n","    print(a)\n","    \n","    b = torch.rand(3,3).cuda()\n","    print(b)\n","\n","    print(a + b)\n","\n","    a = a.cpu() # Error expected\n","    print(a + b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[0.5274, 0.6325, 0.0910],\n","        [0.2323, 0.7269, 0.1187],\n","        [0.3951, 0.7199, 0.7595]], device='cuda:0')\n","tensor([[0.5311, 0.6449, 0.7224],\n","        [0.4416, 0.3634, 0.8818],\n","        [0.9874, 0.7316, 0.2814]], device='cuda:0')\n","tensor([[1.0585, 1.2775, 0.8134],\n","        [0.6739, 1.0903, 1.0006],\n","        [1.3825, 1.4515, 1.0409]], device='cuda:0')\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"expected type torch.FloatTensor but got torch.cuda.FloatTensor","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-82-0cfe18366dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Error expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: expected type torch.FloatTensor but got torch.cuda.FloatTensor"]}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"3O4dgIkBM85n"},"source":["### Exercises\n","\n","Some of these exercises require operations not covered in the notebook.  You will have to look at [the documentation](https://pytorch.org/docs/) (on purpose!)\n","\n","\n","(Answers are at the bottom)"]},{"cell_type":"markdown","metadata":{"id":"K2Rse-GNM85n"},"source":["#### Exercise 1\n","\n","Create a 2D tensor and then add a dimension of size 1 inserted at the 0th axis."]},{"cell_type":"code","metadata":{"id":"4-KiuoLxM85n"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0WgJbOzM85o"},"source":["#### Exercise 2\n","\n","Remove the extra dimension you just added to the previous tensor."]},{"cell_type":"code","metadata":{"id":"SRtQ6urDM85o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlP8NLjNM85o"},"source":["#### Exercise 3\n","\n","Create a random tensor of shape 5x3 in the interval [3, 7)"]},{"cell_type":"code","metadata":{"id":"aKAoLQ_aM85o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fC_TkpmCM85o"},"source":["#### Exercise 4\n","\n","Create a tensor with values from a normal distribution (mean=0, std=1)."]},{"cell_type":"code","metadata":{"id":"UA_3ttTiM85o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d1C5oFy-M85p"},"source":["#### Exercise 5\n","\n","Retrieve the indexes of all the non zero elements in the tensor torch.Tensor([1, 1, 1, 0, 1])."]},{"cell_type":"code","metadata":{"id":"_A5cpDJcM85p"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J26GpKCsM85p"},"source":["#### Exercise 6\n","\n","Create a random tensor of size (3,1) and then horizonally stack 4 copies together."]},{"cell_type":"code","metadata":{"id":"qrKusbeEM85p"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3EVffFVKM85p"},"source":["#### Exercise 7\n","\n","Return the batch matrix-matrix product of two 3 dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4))."]},{"cell_type":"code","metadata":{"id":"n-7BgrbbM85p"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GSN-Nk3PM85q"},"source":["#### Exercise 8\n","\n","Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4))."]},{"cell_type":"code","metadata":{"id":"mK_4wSPEM85q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tRYSi13bM85q"},"source":["Answers below"]},{"cell_type":"code","metadata":{"id":"GidBKzxHM85q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ea14aj73M85q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QH6OdmrqM85q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKlGHp0dM85q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zSAbdNtpM85r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U44t0FQAM85r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IT1zDNHpM85r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L_6WnWZvM85r"},"source":["Answers still below.. Keep Going"]},{"cell_type":"code","metadata":{"id":"LX5PBUBoM85r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"onKIOPaPM85r"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JxmEhfMWM85s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgjkOoM2M85s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mq_qjkhfM85s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jl1WTvB5M85s"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"li75nTTBM85s"},"source":["#### Exercise 1\n","\n","Create a 2D tensor and then add a dimension of size 1 inserted at the 0th axis."]},{"cell_type":"code","metadata":{"id":"3qUgC91qM85t"},"source":["a = torch.rand(3,3)\n","a = a.unsqueeze(0)\n","print(a)\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Tzt1qrqCM85t"},"source":["#### Exercise 2 \n","\n","Remove the extra dimension you just added to the previous tensor."]},{"cell_type":"code","metadata":{"id":"oo9T_--_M85t"},"source":["a = a.squeeze(0)\n","print(a.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"p-Gf6i13M85t"},"source":["#### Exercise 3\n","\n","Create a random tensor of shape 5x3 in the interval [3, 7)"]},{"cell_type":"code","metadata":{"id":"Xb96-gNWM85u"},"source":["3 + torch.rand(5, 3) * 4"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"dyvJZJ9YM85u"},"source":["#### Exercise 4\n","\n","Create a tensor with values from a normal distribution (mean=0, std=1)."]},{"cell_type":"code","metadata":{"id":"JPR9yEShM85u"},"source":["a = torch.rand(3,3)\n","a.normal_(mean=0, std=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nj0pKJrM85u"},"source":["#### Exercise 5\n","\n","Retrieve the indexes of all the non zero elements in the tensor torch.Tensor([1, 1, 1, 0, 1])."]},{"cell_type":"code","metadata":{"id":"wCwgLlsBM85u"},"source":["a = torch.Tensor([1, 1, 1, 0, 1])\n","torch.nonzero(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YQhd6RW_M85v"},"source":["#### Exercise 6\n","\n","Create a random tensor of size (3,1) and then horizonally stack 4 copies together."]},{"cell_type":"code","metadata":{"id":"ho2nMiqpM85v"},"source":["a = torch.rand(3,1)\n","a.expand(3,4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y_h-BhlqM85v"},"source":["#### Exercise 7\n","\n","Return the batch matrix-matrix product of two 3 dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4))."]},{"cell_type":"code","metadata":{"id":"dJtONDxrM85v"},"source":["a = torch.rand(3,4,5)\n","b = torch.rand(3,5,4)\n","torch.bmm(a, b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"CGOZX8KiM85w"},"source":["#### Exercise 8\n","\n","Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4))."]},{"cell_type":"code","metadata":{"id":"jMuwws_iM85w"},"source":["a = torch.rand(3,4,5)\n","b = torch.rand(5,4)\n","torch.bmm(a, b.unsqueeze(0).expand(a.size(0), *b.size()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"0-IYZNP-M85w"},"source":["### END"]}]}